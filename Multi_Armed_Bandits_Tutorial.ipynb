{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOyFska4WfiFoGYMUnssd09",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DennisSoemers/MultiArmedBanditsTutorial/blob/main/Multi_Armed_Bandits_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-Armed Bandits Tutorial\n",
        "If you opened this notebook in Google Colab, I recommend to start by saving a copy of the notebook in your own Google Drive, such that you can save any of your changes and experiments."
      ],
      "metadata": {
        "id": "3y8i6RZUsnm1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Modules\n",
        "We will start by importing some modules that will be useful throughout much of the subsequent code in this tutorial."
      ],
      "metadata": {
        "id": "dYIEsbx8sYoS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "8tYSYUkcgu47"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "from math import sqrt\n",
        "from scipy.stats import norm\n",
        "from typing import List\n",
        "\n",
        "# Make plots look nice\n",
        "sns.set()\n",
        "sns.set_context(\"notebook\")\n",
        "sns.set_style(\"whitegrid\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating the Stochastic MAB Problem\n",
        "\n",
        "In the real world, we normally don't have direct access to the distributions over rewards that are associated with our actions (arms). Normally, these distributions implicitly exist somewhere, and our reward observations only emerge as we interact with some processes (e.g., users, complex simulations, etc.).\n",
        "\n",
        "For simplicity, here we just create a bunch of explicit distributions that we can sample from. This gives us a very simple simulation, and lets us focus purely on the implementation of the action-selection algorithms."
      ],
      "metadata": {
        "id": "lFdBYlaI4Iir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(2077)    # fix seed to make this reproducible\n",
        "k = 25      # number of arms\n",
        "\n",
        "# In this section, we'll generate Normal distributions for all of our arms,\n",
        "# because that's simple and easy. Note that in principle, it is not necessarily\n",
        "# the case that every arm has a normal distribution (or the same type of\n",
        "# distribution).\n",
        "#\n",
        "# Every normal distribution is defined by a mean and a standard deviation.\n",
        "# We'll sample random values for our means and standard deviations, such that\n",
        "# even we ourselves don't know which arms are best (at least, not without\n",
        "# cheating and looking at the means that were generated).\n",
        "\n",
        "# List with one tuple for every arm. First number of every tuple is mean,\n",
        "# second is standard deviation\n",
        "reward_distributions = [(np.random.random() - 0.5, 0.5 * np.random.random()) for _ in range(k)]"
      ],
      "metadata": {
        "id": "OQEbR72P6ryd"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random MAB Algorithm\n",
        "\n",
        "As a start, we'll implement a very simple and silly Random MAB algorithm, which always selects arms uniformly at random. It is not intelligent at all, but it is very easy example, and a useful sanity check: if any of our algorithms every do worse than this, something is probably wrong!"
      ],
      "metadata": {
        "id": "sFhCx3eLLwtR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomAlg:\n",
        "  \"\"\"\n",
        "  Random MAB algorithm, which selects actions uniformly at random.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    \"\"\"\n",
        "    This is where we could initialise any variables we wanted to,\n",
        "    but for the random algorithm this is not necessary, so we do\n",
        "    nothing.\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "  def choose_arm(self) -> int:\n",
        "    \"\"\"\n",
        "    :return: Arm, in [0, k), selected uniformly at random.\n",
        "    \"\"\"\n",
        "    return np.random.randint(low=0, high=k)\n",
        "\n",
        "  def observe_reward(self, arm: int, reward: float) -> None:\n",
        "    \"\"\"\n",
        "    This function lets us observe rewards from arms we have selected.\n",
        "    The simple random algorithm doesn't care and does nothing.\n",
        "\n",
        "    :param arm: Index (starting at 0) of the arm we played.\n",
        "    :param reward: The reward we received.\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "  def __str__(self):\n",
        "    return \"Random\"\n"
      ],
      "metadata": {
        "id": "Iq-Y4opGMIGy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simulation Function\n",
        "\n",
        "Next, we'll define a function that runs a simulation for a single MAB algorithm. A simulation means: running a sequence of time steps, where we ask the algorithm to pick an arm in every time step, and we sample a reward from the distribution of the arm that was picked. **You should not have to change this!**"
      ],
      "metadata": {
        "id": "kFK6pUVHSKX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_simulation(algorithm, n: int = 20000) -> List[float]:\n",
        "  \"\"\"\n",
        "  Runs a simulation for a single algorithm and a given number of time steps.\n",
        "\n",
        "  :param algorithm: Algorithm to use to select arms.\n",
        "  :param n: Number of time steps we'll simulate.\n",
        "  :return: List of rewards we have obtained.\n",
        "  \"\"\"\n",
        "  rewards = list()\n",
        "\n",
        "  for t in range(n):\n",
        "    arm = algorithm.choose_arm()\n",
        "\n",
        "    # NOTE: it's unrealistic that we're explicitly getting mean and std of a\n",
        "    # reward distribution here! In the real world, this distribution would\n",
        "    # emerge from some natural process or complex simulation that we can\n",
        "    # only interact with through sampling!\n",
        "    mean, std = reward_distributions[arm]\n",
        "    reward = norm.rvs(loc=mean, scale=std, size=1)\n",
        "\n",
        "    rewards.append(reward)\n",
        "\n",
        "  return rewards"
      ],
      "metadata": {
        "id": "TcHbQUi4TufD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## First Experiment: Evaluating the Random Algorithm\n",
        "As a first experiment, we will evaluate the performance (in terms of rewards and cumulative regret) of the Random algorithm."
      ],
      "metadata": {
        "id": "N6-kK242Y71A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First we have to run the simulation with the Random algorithm\n",
        "rewards = run_simulation(RandomAlg())\n",
        "rewards = np.asarray(rewards)   # numpy array nicer to work with than list\n",
        "\n",
        "# We can print the sum of rewards obtained\n",
        "print(f\"Total rewards = {np.sum(rewards)}\")\n",
        "\n",
        "# Create a plot of the rewards we collect over time\n",
        "palette = itertools.cycle(sns.color_palette('colorblind'))\n",
        "fig, ax = plt.subplots()\n",
        "plt.plot([t for t in range(len(rewards))], np.cumsum(rewards), label=\"Random\", color=next(palette))\n",
        "ax.set_xlabel(\"Time\")\n",
        "ax.set_ylabel(\"Cumulative rewards\")\n",
        "ax.legend()"
      ],
      "metadata": {
        "id": "3RyV_0E5ZQ6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing More Algorithms\n",
        "\n",
        "Here, we'll finally start implementing some more advanced algorithms. This is where you'll have to add your own implementations. Every time after you have implemented one algorithm, you can skip to the [Evaluating Stochastic MAB Algorithms](#evaluating-stochastic-mab-algorithms) section down below and add your new algorithm to the list of algorithms that we evaluate."
      ],
      "metadata": {
        "id": "70cQKst0mMBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ExploreThenCommit:\n",
        "  \"\"\"\n",
        "  The explore-then-commit (ETC) algorithm. It should first try every\n",
        "  arm m times (for some m >= 1), and afterwards always greedly pull\n",
        "  whichever arm performed best on average during the initial\n",
        "  exploration phase.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, m: int):\n",
        "    \"\"\"\n",
        "    :param m: Number of times we want to explore every arm.\n",
        "    \"\"\"\n",
        "    self.m = m\n",
        "\n",
        "  def choose_arm(self) -> int:\n",
        "    \"\"\"\n",
        "    :return: Arm, in [0, k).\n",
        "    \"\"\"\n",
        "    # TODO: provide implementation\n",
        "\n",
        "  def observe_reward(self, arm: int, reward: float) -> None:\n",
        "    \"\"\"\n",
        "    This function lets us observe rewards from arms we have selected.\n",
        "\n",
        "    :param arm: Index (starting at 0) of the arm we played.\n",
        "    :param reward: The reward we received.\n",
        "    \"\"\"\n",
        "    # TODO: provide implementation\n",
        "\n",
        "  def __str__(self):\n",
        "    return f\"ETC({self.m})\""
      ],
      "metadata": {
        "id": "66L3DFsM1ZG6"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UCB1:\n",
        "  \"\"\"\n",
        "  The UCB1 algorithm.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, C: float):\n",
        "    \"\"\"\n",
        "    :param C: The exploration parameter C.\n",
        "    \"\"\"\n",
        "    self.C = C\n",
        "    # TODO create variables to memorise visit counts and average (or total) rewards\n",
        "\n",
        "  def choose_arm(self) -> int:\n",
        "    \"\"\"\n",
        "    :return: Arm, in [0, k).\n",
        "    \"\"\"\n",
        "    # TODO: provide implementation\n",
        "\n",
        "  def observe_reward(self, arm: int, reward: float) -> None:\n",
        "    \"\"\"\n",
        "    This function lets us observe rewards from arms we have selected.\n",
        "\n",
        "    :param arm: Index (starting at 0) of the arm we played.\n",
        "    :param reward: The reward we received.\n",
        "    \"\"\"\n",
        "    # TODO: provide implementation\n",
        "\n",
        "  def __str__(self):\n",
        "    return f\"UCB1({self.c:.3f})\""
      ],
      "metadata": {
        "id": "XFHyuTDh2uy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"evaluating-stochastic-mab-algorithms\"></a>\n",
        "## Evaluating Stochastic MAB Algorithms\n",
        "\n",
        "This time we'll not just evaluate the Random algorithm, but also evaluate any new algorithms we've implemented."
      ],
      "metadata": {
        "id": "TlA9WyNLz5hO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of all the algorithms we want to evaluate\n",
        "algorithms = [\n",
        "    RandomAlg(),\n",
        "\n",
        "    # Uncomment once you've implemented this algorithm:\n",
        "    ExploreThenCommit(m=20),\n",
        "\n",
        "    # Uncomment once you've implemented this algorithm:\n",
        "    UCB1(C=sqrt(2.0)),\n",
        "]\n",
        "\n",
        "# Prepare fig\n",
        "palette = itertools.cycle(sns.color_palette('colorblind'))\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "# For every algorithm, run simulation and add plot\n",
        "for algorithm in algorithms:\n",
        "  rewards = run_simulation(algorithm)\n",
        "  rewards = np.asarray(rewards)\n",
        "  plt.plot([t for t in range(len(rewards))], np.cumsum(rewards), label=algorithm.__str__(), color=next(palette))\n",
        "\n",
        "# Finish fig\n",
        "ax.set_xlabel(\"Time\")\n",
        "ax.set_ylabel(\"Cumulative rewards\")\n",
        "ax.legend()"
      ],
      "metadata": {
        "id": "8a7oHMRO7TVC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}