{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNj/otXYJv3P0YpPTW6G0Xc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DennisSoemers/MultiArmedBanditsTutorial/blob/main/Multi_Armed_Bandits_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-Armed Bandits Tutorial\n",
        "If you opened this notebook in Google Colab, I recommend to start by saving a copy of the notebook in your own Google Drive, such that you can save any of your changes and experiments."
      ],
      "metadata": {
        "id": "3y8i6RZUsnm1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Modules\n",
        "We will start by importing some modules that will be useful throughout much of the subsequent code in this tutorial."
      ],
      "metadata": {
        "id": "dYIEsbx8sYoS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8tYSYUkcgu47"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "from math import log, sqrt\n",
        "from scipy.stats import norm\n",
        "from tqdm import tqdm\n",
        "from typing import List\n",
        "\n",
        "# Make plots look nice\n",
        "sns.set()\n",
        "sns.set_context(\"notebook\")\n",
        "sns.set_style(\"whitegrid\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating the Stochastic MAB Problems\n",
        "\n",
        "In the real world, we normally don't have direct access to the distributions over rewards that are associated with our actions (arms). Normally, these distributions implicitly exist somewhere, and our reward observations only emerge as we interact with some processes (e.g., users, complex simulations, etc.).\n",
        "\n",
        "For simplicity, here we just create a bunch of explicit distributions that we can sample from. This gives us a very simple simulation, and lets us focus purely on the implementation of the action-selection algorithms.\n",
        "\n",
        "More specifically, we create a suite of 200 different MAB problems, where each\n",
        "individual MAB problem has $k = 10$ arms. Every arm has a mean reward $\\mu_i$\n",
        "which is sampled from a Normal distribution with mean $0$ and unit variance.\n",
        "Then, every arm itself also again samples its rewards from a Normal distribution\n",
        "with its own mean $\\mu_i$, and unit variance. This test setup is the 10-armed\n",
        "Testbed as described in Section 2.3 of the second edition of Sutton and Barto's *Reinforcement Learning* book, except we use only 200 instead of 2000 MABs (to make our experiments go faster)."
      ],
      "metadata": {
        "id": "lFdBYlaI4Iir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(2077)    # fix seed to make everything below reproducible\n",
        "k = 10      # number of arms per MAB problem\n",
        "num_mab_problems = 200\n",
        "time_steps_per_problem = 1000\n",
        "\n",
        "# Matrix of means, where the i'th row contains the means of all arms for the\n",
        "# i'th MAB problem\n",
        "arm_means = np.random.normal(loc=0.0, scale=1.0, size=(num_mab_problems, k))"
      ],
      "metadata": {
        "id": "OQEbR72P6ryd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random MAB Algorithm\n",
        "\n",
        "As a start, we'll implement a very simple and silly Random MAB algorithm, which always selects arms uniformly at random. It is not intelligent at all, but it is very easy example, and a useful sanity check: if any of our algorithms ever do worse than this, something is probably wrong!"
      ],
      "metadata": {
        "id": "sFhCx3eLLwtR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomAlg:\n",
        "  \"\"\"\n",
        "  Random MAB algorithm, which selects actions uniformly at random.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    \"\"\"\n",
        "    This is where we could initialise any variables we wanted to,\n",
        "    but for the random algorithm this is not necessary, so we do\n",
        "    nothing.\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "  def reset(self) -> None:\n",
        "    \"\"\"\n",
        "    Reset any data that we stored. Called when we start with a\n",
        "    new MAB problem. The random algorithm doesn't store any\n",
        "    data, so nothing to reset here.\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "  def choose_arm(self) -> int:\n",
        "    \"\"\"\n",
        "    :return: Arm, in [0, k), selected uniformly at random.\n",
        "    \"\"\"\n",
        "    return np.random.randint(low=0, high=k)\n",
        "\n",
        "  def observe_reward(self, arm: int, reward: float) -> None:\n",
        "    \"\"\"\n",
        "    This function lets us observe rewards from arms we have selected.\n",
        "    The simple random algorithm doesn't care and does nothing.\n",
        "\n",
        "    :param arm: Index (starting at 0) of the arm we played.\n",
        "    :param reward: The reward we received.\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "  def __str__(self):\n",
        "    return \"Random\"\n"
      ],
      "metadata": {
        "id": "Iq-Y4opGMIGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simulation Function\n",
        "\n",
        "Next, we'll define a function that runs a simulation for a single MAB algorithm on a single MAB problem. A simulation means: running a sequence of time steps, where we ask the algorithm to pick an arm in every time step, and we sample a reward from the distribution of the arm that was picked. **You should not have to change this!**"
      ],
      "metadata": {
        "id": "kFK6pUVHSKX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_simulation(algorithm, true_arm_means, n: int = 1000) -> List[float]:\n",
        "  \"\"\"\n",
        "  Runs a simulation for a single algorithm and a given number of time steps.\n",
        "\n",
        "  :param algorithm: Algorithm to use to select arms.\n",
        "  :param true_arm_means: Vector of true mean rewards for all arms.\n",
        "  :param n: Number of time steps we'll simulate.\n",
        "  :return: List of rewards we have obtained.\n",
        "  \"\"\"\n",
        "  rewards = np.zeros(n)\n",
        "  algorithm.reset()\n",
        "\n",
        "  for t in range(n):\n",
        "    arm = algorithm.choose_arm()\n",
        "\n",
        "    # NOTE: it's unrealistic that we're explicitly using mean and std of a\n",
        "    # reward distribution here! In the real world, this distribution would\n",
        "    # emerge from some natural process or complex simulation that we can\n",
        "    # only interact with through sampling!\n",
        "    reward = norm.rvs(loc=true_arm_means[arm], scale=1.0)\n",
        "\n",
        "    algorithm.observe_reward(arm, reward)\n",
        "    rewards[t] = reward\n",
        "\n",
        "  return rewards"
      ],
      "metadata": {
        "id": "TcHbQUi4TufD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## First Experiment: Evaluating the Random Algorithm\n",
        "As a first experiment, we will evaluate the performance (in terms of rewards) of the Random algorithm."
      ],
      "metadata": {
        "id": "N6-kK242Y71A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Allocate memory to store rewards collected for all MAB problems\n",
        "rewards_matrix = np.zeros(shape=(num_mab_problems, time_steps_per_problem))\n",
        "\n",
        "# Loop through all our MAB problems\n",
        "for problem in tqdm(range(num_mab_problems), desc=\"Evaluating Random algorithm\"):\n",
        "  # Run a simulation with the Random algorithm in this problem\n",
        "  rewards_matrix[problem, :] = run_simulation(algorithm=RandomAlg(), true_arm_means=arm_means[problem, :], n=time_steps_per_problem)\n",
        "\n",
        "# Create a plot of the cumulative rewards we collected over time, averaged over all MAB problems\n",
        "cumulative_rewards_per_prob = np.cumsum(rewards_matrix, axis=1, dtype=np.float64)\n",
        "\n",
        "palette = itertools.cycle(sns.color_palette('colorblind'))\n",
        "fig, ax = plt.subplots()\n",
        "plt.plot([t for t in range(time_steps_per_problem)], np.mean(cumulative_rewards_per_prob, axis=0), label=\"Random\", color=next(palette))\n",
        "ax.set_xlabel(\"Time\")\n",
        "ax.set_ylabel(f\"Cum. rewards (averaged over {num_mab_problems} MABs)\")\n",
        "ax.legend()"
      ],
      "metadata": {
        "id": "3RyV_0E5ZQ6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing More Algorithms\n",
        "\n",
        "Here, we'll finally start implementing some more advanced algorithms. This is where you'll have to add your own implementations. Every time after you have implemented one algorithm, you can skip to the [Evaluating Stochastic MAB Algorithms](#evaluating-stochastic-mab-algorithms) section down below and add your new algorithm to the list of algorithms that we evaluate."
      ],
      "metadata": {
        "id": "70cQKst0mMBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ExploreThenCommit:\n",
        "  \"\"\"\n",
        "  The explore-then-commit (ETC) algorithm. It should first try every\n",
        "  arm m times (for some m >= 1), and afterwards always greedly pull\n",
        "  whichever arm performed best on average during the initial\n",
        "  exploration phase.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, m: int):\n",
        "    \"\"\"\n",
        "    :param m: Number of times we want to explore every arm.\n",
        "    \"\"\"\n",
        "    self.m = m\n",
        "    # TODO: init any other variables for memory you may need here\n",
        "\n",
        "  def reset(self) -> None:\n",
        "    \"\"\"\n",
        "    Reset all memory.\n",
        "    \"\"\"\n",
        "    # TODO: provide implementation\n",
        "\n",
        "  def choose_arm(self) -> int:\n",
        "    \"\"\"\n",
        "    :return: Arm, in [0, k).\n",
        "    \"\"\"\n",
        "    # TODO: provide implementation\n",
        "\n",
        "  def observe_reward(self, arm: int, reward: float) -> None:\n",
        "    \"\"\"\n",
        "    This function lets us observe rewards from arms we have selected.\n",
        "\n",
        "    :param arm: Index (starting at 0) of the arm we played.\n",
        "    :param reward: The reward we received.\n",
        "    \"\"\"\n",
        "    # TODO: provide implementation\n",
        "\n",
        "  def __str__(self):\n",
        "    return f\"ETC({self.m})\""
      ],
      "metadata": {
        "id": "66L3DFsM1ZG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EpsilonGreedy:\n",
        "  \"\"\"\n",
        "  The epsilon-greedy algorithm.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, epsilon: float):\n",
        "    \"\"\"\n",
        "    :param epsilon: The epsilon parameter (probability of selecting random arm).\n",
        "    \"\"\"\n",
        "    self.epsilon = epsilon\n",
        "    # TODO: init any other variables for memory you may need here\n",
        "\n",
        "  def reset(self) -> None:\n",
        "    \"\"\"\n",
        "    Reset all memory.\n",
        "    \"\"\"\n",
        "    # TODO: provide implementation\n",
        "\n",
        "  def choose_arm(self) -> int:\n",
        "    \"\"\"\n",
        "    :return: Arm, in [0, k).\n",
        "    \"\"\"\n",
        "    # TODO: provide implementation\n",
        "\n",
        "  def observe_reward(self, arm: int, reward: float) -> None:\n",
        "    \"\"\"\n",
        "    This function lets us observe rewards from arms we have selected.\n",
        "\n",
        "    :param arm: Index (starting at 0) of the arm we played.\n",
        "    :param reward: The reward we received.\n",
        "    \"\"\"\n",
        "    # TODO: provide implementation\n",
        "\n",
        "  def __str__(self):\n",
        "    return f\"eps-greedy({self.epsilon:.3f})\""
      ],
      "metadata": {
        "id": "rS0Az8Sa6vcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UCB1:\n",
        "  \"\"\"\n",
        "  The UCB1 algorithm.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, C: float):\n",
        "    \"\"\"\n",
        "    :param C: The exploration parameter C.\n",
        "    \"\"\"\n",
        "    self.C = C\n",
        "    # TODO: init any other variables for memory you may need here\n",
        "\n",
        "  def reset(self) -> None:\n",
        "    \"\"\"\n",
        "    Reset all memory.\n",
        "    \"\"\"\n",
        "    # TODO: provide implementation\n",
        "\n",
        "  def choose_arm(self) -> int:\n",
        "    \"\"\"\n",
        "    :return: Arm, in [0, k).\n",
        "    \"\"\"\n",
        "    # TODO: provide implementation\n",
        "\n",
        "  def observe_reward(self, arm: int, reward: float) -> None:\n",
        "    \"\"\"\n",
        "    This function lets us observe rewards from arms we have selected.\n",
        "\n",
        "    :param arm: Index (starting at 0) of the arm we played.\n",
        "    :param reward: The reward we received.\n",
        "    \"\"\"\n",
        "    # TODO: provide implementation\n",
        "\n",
        "  def __str__(self):\n",
        "    return f\"UCB1({self.C:.3f})\""
      ],
      "metadata": {
        "id": "XFHyuTDh2uy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GradientBandit:\n",
        "  \"\"\"\n",
        "  The Stochastic Gradient Bandit algorithm.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, alpha: float):\n",
        "    \"\"\"\n",
        "    :param alpha: The learning rate alpha.\n",
        "    \"\"\"\n",
        "    self.alpha = alpha\n",
        "    # TODO: init any other variables for memory you may need here\n",
        "\n",
        "  def reset(self) -> None:\n",
        "    \"\"\"\n",
        "    Reset all memory.\n",
        "    \"\"\"\n",
        "    # TODO: provide implementation\n",
        "\n",
        "  def choose_arm(self) -> int:\n",
        "    \"\"\"\n",
        "    :return: Arm, in [0, k).\n",
        "    \"\"\"\n",
        "    # TODO: provide implementation\n",
        "\n",
        "  def observe_reward(self, arm: int, reward: float) -> None:\n",
        "    \"\"\"\n",
        "    This function lets us observe rewards from arms we have selected.\n",
        "\n",
        "    :param arm: Index (starting at 0) of the arm we played.\n",
        "    :param reward: The reward we received.\n",
        "    \"\"\"\n",
        "    # TODO: provide implementation\n",
        "\n",
        "  def __str__(self):\n",
        "    return f\"Gradient Bandit({self.alpha:.3f})\""
      ],
      "metadata": {
        "id": "NTi0YlytBpHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GradientBanditBaseline:\n",
        "  \"\"\"\n",
        "  The Stochastic Gradient Bandit algorithm, with baseline.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, alpha: float):\n",
        "    \"\"\"\n",
        "    :param alpha: The learning rate alpha.\n",
        "    \"\"\"\n",
        "    self.alpha = alpha\n",
        "    # TODO: init any other variables for memory you may need here\n",
        "\n",
        "  def reset(self) -> None:\n",
        "    \"\"\"\n",
        "    Reset all memory.\n",
        "    \"\"\"\n",
        "    # TODO: provide implementation\n",
        "\n",
        "  def choose_arm(self) -> int:\n",
        "    \"\"\"\n",
        "    :return: Arm, in [0, k).\n",
        "    \"\"\"\n",
        "    # TODO: provide implementation\n",
        "\n",
        "  def observe_reward(self, arm: int, reward: float) -> None:\n",
        "    \"\"\"\n",
        "    This function lets us observe rewards from arms we have selected.\n",
        "\n",
        "    :param arm: Index (starting at 0) of the arm we played.\n",
        "    :param reward: The reward we received.\n",
        "    \"\"\"\n",
        "    # TODO: provide implementation\n",
        "\n",
        "  def __str__(self):\n",
        "    return f\"Gradient Bandit with Baseline({self.alpha:.3f})\""
      ],
      "metadata": {
        "id": "Yt8-BK35B-l4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"evaluating-stochastic-mab-algorithms\"></a>\n",
        "## Evaluating Stochastic MAB Algorithms\n",
        "\n",
        "This time we'll not just evaluate the Random algorithm, but also evaluate any new algorithms we've implemented. We'll also add a second type of plot: one that shows the average reward"
      ],
      "metadata": {
        "id": "TlA9WyNLz5hO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of all the algorithms we want to evaluate\n",
        "algorithms = [\n",
        "    RandomAlg(),\n",
        "\n",
        "    # Uncomment once you've implemented this algorithm:\n",
        "    #ExploreThenCommit(m=10),\n",
        "\n",
        "    # Uncomment once you've implemented this algorithm:\n",
        "    #EpsilonGreedy(epsilon=0.1),\n",
        "\n",
        "    # Uncomment once you've implemented this algorithm:\n",
        "    #UCB1(C=sqrt(2.0)),\n",
        "\n",
        "    # Uncomment once you've implemented this algorithm:\n",
        "    #GradientBandit(alpha=0.1),\n",
        "\n",
        "    # Uncomment once you've implemented this algorithm:\n",
        "    #GradientBanditBaseline(alpha=0.1),\n",
        "]\n",
        "\n",
        "# Define a function that creates evaluation plots for a list of algorithms.\n",
        "def create_eval_plots_for_algs(algorithms):\n",
        "  # Prepare figures\n",
        "  palette = itertools.cycle(sns.color_palette('colorblind'))\n",
        "  linestyles = itertools.cycle([\"-\", \"--\", \":\"])\n",
        "  fig_cumsums, ax_cumsums = plt.subplots()\n",
        "  fig_avgs, ax_avgs = plt.subplots()\n",
        "\n",
        "  # For every algorithm, run simulation and add plot\n",
        "  for algorithm in algorithms:\n",
        "    color = next(palette)\n",
        "    linestyle = next(linestyles)\n",
        "\n",
        "    # Allocate memory to store rewards collected for all MAB problems\n",
        "    rewards_matrix = np.zeros(shape=(num_mab_problems, time_steps_per_problem))\n",
        "\n",
        "    # Loop through all our MAB problems\n",
        "    for problem in tqdm(range(num_mab_problems), desc=\"Evaluating \" + algorithm.__str__()):\n",
        "      # Run a simulation with the Random algorithm in this problem\n",
        "      rewards_matrix[problem, :] = run_simulation(algorithm=algorithm, true_arm_means=arm_means[problem, :], n=time_steps_per_problem)\n",
        "\n",
        "    # Create a plot of the cumulative rewards we collected over time, averaged over all MAB problems\n",
        "    cumulative_rewards_per_prob = np.cumsum(rewards_matrix, axis=1, dtype=np.float64)\n",
        "    ax_cumsums.plot([t for t in range(time_steps_per_problem)], np.mean(cumulative_rewards_per_prob, axis=0),\n",
        "                    label=algorithm.__str__(), color=color, linestyle=linestyle)\n",
        "\n",
        "    # Create a plot of the average (over all timesteps so far) rewards collected per time step, averaged over all MAB problems\n",
        "    cumulative_means_per_prob = cumulative_rewards_per_prob / np.arange(1, time_steps_per_problem + 1)\n",
        "    ax_avgs.plot([t for t in range(time_steps_per_problem)], np.mean(cumulative_means_per_prob, axis=0),\n",
        "                label=algorithm.__str__(), color=color, linestyle=linestyle)\n",
        "\n",
        "  # Finish figures\n",
        "  ax_cumsums.set_xlabel(\"Time\")\n",
        "  ax_cumsums.set_ylabel(f\"Cum. rewards (averaged over {num_mab_problems} MABs)\")\n",
        "  ax_cumsums.legend()\n",
        "\n",
        "  ax_avgs.set_xlabel(\"Time\")\n",
        "  ax_avgs.set_ylabel(f\"Avg. rewards (averaged over {num_mab_problems} MABs)\")\n",
        "  ax_avgs.legend()\n",
        "\n",
        "# Call the function we just defined\n",
        "create_eval_plots_for_algs(algorithms)"
      ],
      "metadata": {
        "id": "8a7oHMRO7TVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating Hyperparameters: ETC\n",
        "\n",
        "Let's evaluate ETC with different values for its hyperparameter."
      ],
      "metadata": {
        "id": "pVwll944qzFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of all the algorithms we want to evaluate\n",
        "etc_algorithms = [\n",
        "    ExploreThenCommit(m=1),\n",
        "    ExploreThenCommit(m=2),\n",
        "    ExploreThenCommit(m=5),\n",
        "    ExploreThenCommit(m=10),\n",
        "    ExploreThenCommit(m=20),\n",
        "]\n",
        "\n",
        "create_eval_plots_for_algs(etc_algorithms)"
      ],
      "metadata": {
        "id": "MDUjZW27q0hM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating Hyperparameters: $\\epsilon$-greedy\n",
        "\n",
        "Let's evaluate $\\epsilon$-greedy with different values for its hyperparameter."
      ],
      "metadata": {
        "id": "cwNYF8DCq3F0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of all the algorithms we want to evaluate\n",
        "eps_greedy_algorithms = [\n",
        "    EpsilonGreedy(epsilon=0.0),\n",
        "    EpsilonGreedy(epsilon=0.05),\n",
        "    EpsilonGreedy(epsilon=0.1),\n",
        "    EpsilonGreedy(epsilon=0.15),\n",
        "    EpsilonGreedy(epsilon=0.2),\n",
        "]\n",
        "\n",
        "create_eval_plots_for_algs(eps_greedy_algorithms)"
      ],
      "metadata": {
        "id": "2y3SEAC1q4_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating Hyperparameters: UCB1\n",
        "\n",
        "Let's evaluate UCB1 with different values for its hyperparameter."
      ],
      "metadata": {
        "id": "zBFcYkpRq7h2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of all the algorithms we want to evaluate\n",
        "ucb1_algorithms = [\n",
        "    UCB1(C=0.0),\n",
        "    UCB1(C=0.1),\n",
        "    UCB1(C=0.6),\n",
        "    UCB1(C=1.0),\n",
        "    UCB1(C=sqrt(2.0)),\n",
        "    UCB1(C=2.0),\n",
        "]\n",
        "\n",
        "create_eval_plots_for_algs(ucb1_algorithms)"
      ],
      "metadata": {
        "id": "B2MbwFgBq9Ax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating Hyperparameters: Gradient Bandit\n",
        "\n",
        "Let's evaluate Gradient Bandit with different values for its hyperparameter."
      ],
      "metadata": {
        "id": "xDRensIWq_Qo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of all the algorithms we want to evaluate\n",
        "gradient_bandit_algorithms = [\n",
        "    GradientBandit(alpha=0.01),\n",
        "    GradientBandit(alpha=0.05),\n",
        "    GradientBandit(alpha=0.1),\n",
        "    GradientBandit(alpha=0.2),\n",
        "    GradientBandit(alpha=0.5),\n",
        "]\n",
        "\n",
        "create_eval_plots_for_algs(gradient_bandit_algorithms)"
      ],
      "metadata": {
        "id": "skzb4De4rAcn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}