{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMxYke6DrlwK4xK3DP64lMm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DennisSoemers/MultiArmedBanditsTutorial/blob/main/Multi_Armed_Bandits_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-Armed Bandits Tutorial\n",
        "If you opened this notebook in Google Colab, I recommend to start by saving a copy of the notebook in your own Google Drive, such that you can save any of your changes and experiments."
      ],
      "metadata": {
        "id": "3y8i6RZUsnm1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Modules\n",
        "We will start by importing some modules that will be useful throughout much of the subsequent code in this tutorial."
      ],
      "metadata": {
        "id": "dYIEsbx8sYoS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "8tYSYUkcgu47"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "from scipy.stats import norm\n",
        "from typing import List"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating the Stochastic MAB problem\n",
        "\n",
        "In the real world, we normally don't have direct access to the distributions over rewards that are associated with our actions (arms). Normally, these distributions implicitly exist somewhere, and our reward observations only emerge as we interact with some processes (e.g., users, complex simulations, etc.).\n",
        "\n",
        "For simplicity, here we just create a bunch of explicit distributions that we can sample from. This gives us a very simple simulation, and lets us focus purely on the implementation of the action-selection algorithms."
      ],
      "metadata": {
        "id": "lFdBYlaI4Iir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(2077)    # fix seed to make this reproducible\n",
        "k = 20      # number of arms\n",
        "\n",
        "# In this section, we'll generate Normal distributions for all of our arms,\n",
        "# because that's simple and easy. Note that in principle, it is not necessarily\n",
        "# the case that every arm has a normal distribution (or the same type of\n",
        "# distribution).\n",
        "#\n",
        "# Every normal distribution is defined by a mean and a standard deviation.\n",
        "# We'll sample random values for our means and standard deviations, such that\n",
        "# even we ourselves don't know which arms are best (at least, not without\n",
        "# cheating and looking at the means that were generated).\n",
        "\n",
        "# List with one tuple for every arm. First number of every tuple is mean,\n",
        "# second is standard deviation\n",
        "reward_distributions = [(np.random.random(), 0.5 * np.random.random()) for _ in range(k)]"
      ],
      "metadata": {
        "id": "OQEbR72P6ryd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random MAB Algorithm\n",
        "\n",
        "As a start, we'll implement a very simple and silly Random MAB algorithm, which always selects arms uniformly at random. It is not intelligent at all, but it is very easy example, and a useful sanity check: if any of our algorithms every do worse than this, something is probably wrong!"
      ],
      "metadata": {
        "id": "sFhCx3eLLwtR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomAlg:\n",
        "  \"\"\"\n",
        "  Random MAB algorithm, which selects actions uniformly at random.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, num_arms: int):\n",
        "    \"\"\"\n",
        "    This is where we could initialise any variables we wanted to,\n",
        "    but for the random algorithm this is not necessary, so we do\n",
        "    nothing.\n",
        "\n",
        "    :param num_arms: Number of arms in our MAB problem.\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "  def choose_arm(self) -> int:\n",
        "    \"\"\"\n",
        "    :return: Arm, between 1 and k (both inclusive), selected\n",
        "      uniformly at random.\n",
        "    \"\"\"\n",
        "    return np.random.randint(low=1, high=k+1)\n",
        "\n",
        "  def observe_reward(self, arm: int, reward: float) -> None:\n",
        "    \"\"\"\n",
        "    This function lets us observe rewards from arms we have selected.\n",
        "    The simple random algorithm doesn't care and does nothing.\n",
        "\n",
        "    :param arm: Index (starting at 1) of the arm we played.\n",
        "    :param reward: The reward we received.\n",
        "    \"\"\"\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "Iq-Y4opGMIGy"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simulation function\n",
        "\n",
        "Next, we'll define a function that runs a simulation for a single MAB algorithm. A simulation means: running a sequence of time steps, where we ask the algorithm to pick an arm in every time step, and we sample a reward from the distribution of the arm that was picked. **You should not have to change this!**"
      ],
      "metadata": {
        "id": "kFK6pUVHSKX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_simulation(algorithm, n: int = 20000) -> List[float]:\n",
        "  \"\"\"\n",
        "  Runs a simulation for a single algorithm and a given number of time steps.\n",
        "\n",
        "  :param algorithm: Algorithm to use to select arms.\n",
        "  :param n: Number of time steps we'll simulate.\n",
        "  :return: List of rewards we have obtained.\n",
        "  \"\"\"\n",
        "  rewards = list()\n",
        "\n",
        "  for t in range(n):\n",
        "    arm = algorithm.choose_arm()\n",
        "\n",
        "    # NOTE: it's unrealistic that we're explicitly getting mean and std of a\n",
        "    # reward distribution here! In the real world, this distribution would\n",
        "    # emerge from some natural process or complex simulation that we can\n",
        "    # only interact with through sampling!\n",
        "    mean, std = reward_distributions[arm - 1]   # subtract 1 due to 0-based indexing\n",
        "    reward = norm.rvs(loc=mean, scale=std, size=1)\n",
        "\n",
        "    rewards.append(reward)\n",
        "\n",
        "  return rewards"
      ],
      "metadata": {
        "id": "TcHbQUi4TufD"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## First Experiment: Evaluating the Random Algorithm\n",
        "As a first experiment, we will evaluate the performance (in terms of rewards and cumulative regret) of the Random algorithm."
      ],
      "metadata": {
        "id": "N6-kK242Y71A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First we have to run the simulation with the Random algorithm\n",
        "rewards = run_simulation(RandomAlg(k))\n",
        "rewards = np.asarray(rewards)   # numpy array nicer to work with than list\n",
        "\n",
        "# We can print the sum of rewards obtained\n",
        "print(f\"Total rewards = {np.sum(rewards)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3RyV_0E5ZQ6M",
        "outputId": "034add9e-d8b7-4ddd-d5b7-dfeca263e63a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total rewards = 8984.283507448963\n"
          ]
        }
      ]
    }
  ]
}